from celery import shared_task
from api.models import Document, DocumentChunk
from api.services.embedding_service import generate_embedding
from api.services.document_processor import extract_text_from_file_content, split_text_into_chunks
from api.services.vector_store import store_document_chunks, retrieve_relevant_chunks
from api.services.llm_service import generate_text_from_llm
from django.db import transaction
import logging

logger = logging.getLogger(__name__)

@shared_task(bind=True, name="process_document_task")
def process_document_task(self, document_id: str, filename: str, file_content: str):
    """
    Celery task to process an uploaded document: extract text, chunk, embed, and store.
    """
    logger.info(f"Starting to process document: {filename} (ID: {document_id})")
    try:
        document_obj = Document.objects.get(id=document_id)

        # 1. Extract text
        text_content = extract_text_from_file_content(filename, file_content)
        document_obj.content = text_content # Store raw content for reference
        document_obj.save(update_fields=['content'])

        # 2. Split into chunks
        chunks = split_text_into_chunks(text_content)
        logger.info(f"Document split into {len(chunks)} chunks.")

        # 3. Generate embeddings for each chunk
        chunks_with_embeddings = []
        for i, chunk_text in enumerate(chunks):
            # You might want to batch embedding generation for performance
            embedding = generate_embedding(chunk_text)
            chunks_with_embeddings.append((chunk_text, embedding))
            if (i + 1) % 10 == 0:
                logger.info(f"Processed {i+1}/{len(chunks)} embeddings for {filename}.")

        # 4. Store chunks and embeddings in vector store (PostgreSQL)
        with transaction.atomic():
            store_document_chunks(document_obj, chunks_with_embeddings)

        logger.info(f"Successfully processed and stored document: {filename}")
        return {"status": "success", "document_id": str(document_id), "chunk_count": len(chunks)}

    except Document.DoesNotExist:
        logger.error(f"Document with ID {document_id} not found.")
        self.update_state(state='FAILURE', meta={'error': 'Document not found'})
        raise
    except Exception as e:
        logger.error(f"Error processing document {filename}: {e}", exc_info=True)
        self.update_state(state='FAILURE', meta={'error': str(e)})
        raise


@shared_task(bind=True, name="answer_question_task")
def answer_question_task(self, question: str, num_chunks: int = 5):
    """
    Celery task to answer a question using RAG: embed query, retrieve chunks, generate answer.
    """
    logger.info(f"Starting to answer question: {question[:50]}...")
    try:
        # 1. Embed the user's question
        query_embedding = generate_embedding(question)
        logger.info("Question embedding generated.")

        # 2. Retrieve relevant chunks from the vector store
        relevant_chunks = retrieve_relevant_chunks(query_embedding, num_chunks=num_chunks)

        if not relevant_chunks:
            answer = "I couldn't find any relevant information in the documents to answer your question."
            logger.warning("No relevant chunks found for the question.")
            return {"status": "success", "question": question, "answer": answer, "retrieved_chunks": []}

        # 3. Construct the prompt for the LLM
        context = "\n".join([chunk.text for chunk in relevant_chunks])
        prompt = f"""You are a helpful assistant. Use the following context to answer the question.
If the answer is not in the context, say "I don't have enough information to answer that question."

Context:
{context}

Question: {question}
Answer:
"""
        logger.info("LLM prompt constructed.")

        # 4. Generate the answer using the LLM
        answer = generate_text_from_llm(prompt)
        logger.info("Answer generated by LLM.")

        return {
            "status": "success",
            "question": question,
            "answer": answer,
            "retrieved_chunks": [chunk.text for chunk in relevant_chunks]
        }

    except Exception as e:
        logger.error(f"Error answering question '{question}': {e}", exc_info=True)
        self.update_state(state='FAILURE', meta={'error': str(e)})
        raise
